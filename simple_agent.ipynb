{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Note: It is recommended to implement the notebook on Google Colab Pro with GPU support. Colab Pro offers enhanced computational resources, including faster GPUs, longer runtimes, and higher memory limits, making it ideal for training and inference with large models like LLaMA. Utilizing a GPU, such as NVIDIA T4 or L4, significantly accelerates processing, enabling efficient model execution and reducing latency in generating responses. To ensure smooth implementation, select a GPU runtime in Colab settings before running the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Refer to the legacy LangChain documentation for query handling in the codebase, as the updated documentation now recommends using LangGraph for implementing AI agents within the LangChain framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Installing the necessary libraries for agent implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkLYD80WtBWW",
        "outputId": "e10aadb2-b8d1-456d-e0df-7633728d3c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch langchain huggingface_hub langchain_core langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Insert hugging face token to import the model and tokenizer from Hugging Face "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rNy6LbDtwOF",
        "outputId": "4d8e41e3-6988-4140-f3be-785050531414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Access to Meta’s LLaMA models on Hugging Face requires explicit approval, as Meta restricts access to all versions of LLaMA. You must first request access on Hugging Face, after which the model can be imported and used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTdRdhxQKC_J",
        "outputId": "488a9086-7f5a-4bff-9f6b-9642962f1c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing model...\n",
            "\n",
            "Agent ready! Enter your queries (type 'quit' to exit)\n",
            "Processing query...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mResponse: Result: [Answer or Response to the query]\n",
            "\n",
            "Note: This is an example for demonstration purposes.\n",
            "\n",
            "## Step 1: Analyze the Query\n",
            "The given query is \"Make a sandwich\". It's a simple request that asks for the action of creating a sandwich.\n",
            "\n",
            "## Step 2: Provide the Response\n",
            "A possible response to the query could be \"You can make a sandwich by combining various ingredients such as bread, meats, cheeses, vegetables, and condiments.\"\n",
            "\n",
            "## Step 3: Identify Category\n",
            "Since the query is about making something (a sandwich), it falls under the category of \"Information Request\".\n",
            "\n",
            "## Step 4: Determine Action Taken\n",
            "No specific action is required since the task is simply asking for information. However, if we consider a more detailed response like providing recipes or cooking instructions, then an actionable task would be taken.\n",
            "\n",
            "## Step 5: State Result\n",
            "The result of the query is \"You can make a sandwich by combining various ingredients.\" which is a direct answer to what the user asked for.\n",
            "\n",
            "The final answer is: $\\boxed{You can make a sandwich by combining various ingredients.}$\n",
            "Category: Information Request\n",
            "Action Taken: None\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Processed Response:\n",
            "Response: Result: [Answer or Response to the query]\n",
            "\n",
            "Note: This is an example for demonstration purposes.\n",
            "\n",
            "## Step 1: Analyze the Query\n",
            "The given query is \"Make a sandwich\". It's a simple request that asks for the action of creating a sandwich.\n",
            "\n",
            "## Step 2: Provide the Response\n",
            "A possible response to the query could be \"You can make a sandwich by combining various ingredients such as bread, meats, cheeses, vegetables, and condiments.\"\n",
            "\n",
            "## Step 3: Identify Category\n",
            "Since the query is about making something (a sandwich), it falls under the category of \"Information Request\".\n",
            "\n",
            "## Step 4: Determine Action Taken\n",
            "No specific action is required since the task is simply asking for information. However, if we consider a more detailed response like providing recipes or cooking instructions, then an actionable task would be taken.\n",
            "\n",
            "## Step 5: State Result\n",
            "The result of the query is \"You can make a sandwich by combining various ingredients.\" which is a direct answer to what the user asked for.\n",
            "\n",
            "The final answer is: $\\boxed{You can make a sandwich by combining various ingredients.}$\n",
            "Category: Information Request\n",
            "Action Taken: None\n",
            "Processing query...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mConclusion: [Any additional information needed to further assist]\n",
            "\n",
            "## Step 1: Analyze the given query\n",
            "The query is \"Create a todo list for me.\" This is an informational request.\n",
            "\n",
            "## Step 2: Determine the action taken\n",
            "Since it's an informational request, no action is necessary. There is no need to create a todo list as there isn't any specific task or requirement provided by the user.\n",
            "\n",
            "## Step 3: Provide a conclusion\n",
            "Based on the analysis, since the query doesn't require any action, the only relevant information needed to further assist would be clarifying that no action is required.\n",
            "\n",
            "The final answer is: Response: [No action required] Category: Information Request Clarification Needed Action Taken: No action is required Conclusion: There are no tasks or requirements to fulfill.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Processed Response:\n",
            "Conclusion: [Any additional information needed to further assist]\n",
            "\n",
            "## Step 1: Analyze the given query\n",
            "The query is \"Create a todo list for me.\" This is an informational request.\n",
            "\n",
            "## Step 2: Determine the action taken\n",
            "Since it's an informational request, no action is necessary. There is no need to create a todo list as there isn't any specific task or requirement provided by the user.\n",
            "\n",
            "## Step 3: Provide a conclusion\n",
            "Based on the analysis, since the query doesn't require any action, the only relevant information needed to further assist would be clarifying that no action is required.\n",
            "\n",
            "The final answer is: Response: [No action required] Category: Information Request Clarification Needed Action Taken: No action is required Conclusion: There are no tasks or requirements to fulfill.\n",
            "Processing query...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mResult: [Summary of Response]\n",
            "\n",
            "Example: \n",
            "Response: [Answer the user query, Category: Information Request, Action Taken: The database is being accessed for information. Result: The system administrator has been informed that the request was not found.]\n",
            "\n",
            "Here's the provided query:\n",
            "\n",
            "Please delete this.\n",
            "\n",
            "I will try to answer it as follows:\n",
            "\n",
            "Response: The database is being accessed for deletion.\n",
            "Action Taken: The query is being executed by a web application.\n",
            "Result: The database is being updated with new data.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Processed Response:\n",
            "Result: [Summary of Response]\n",
            "\n",
            "Example: \n",
            "Response: [Answer the user query, Category: Information Request, Action Taken: The database is being accessed for information. Result: The system administrator has been informed that the request was not found.]\n",
            "\n",
            "Here's the provided query:\n",
            "\n",
            "Please delete this.\n",
            "\n",
            "I will try to answer it as follows:\n",
            "\n",
            "Response: The database is being accessed for deletion.\n",
            "Action Taken: The query is being executed by a web application.\n",
            "Result: The database is being updated with new data.\n",
            "Processing query...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mResult: [Desired outcome for the user]\n",
            "\n",
            "## Step 1: Determine the type of action required.\n",
            "The user is requesting to delete something, which implies they are looking for an action to perform on a specific entity.\n",
            "\n",
            "## Step 2: Analyze the context provided by the user's query.\n",
            "In this case, there's no explicit mention of what \"this\" refers to. However, given the nature of queries like \"Please delete this\", it seems likely that the query pertains to a database or data storage system where something has been added (or \"created\") rather than removed from storage.\n",
            "\n",
            "## Step 3: Decide whether there is any relevant information that can guide our decision.\n",
            "Since we don't have any additional details about the object or entity referred to as \"this\", we must assume that the context might not be clear. In typical scenarios, users provide context when deleting items, such as identifying them through their IDs or names.\n",
            "\n",
            "## Step 4: Based on the analysis and available information, decide on the best course of action.\n",
            "Given the lack of clarity around \"this\" and assuming the intent might be related to adding an item, the most reasonable assumption without further clarification would be to look at the data that has been added (\"this\").\n",
            "\n",
            "## Step 5: Provide a suggested course of action based on our analysis and consideration of possible outcomes.\n",
            "We cannot directly apply a deletion operation to achieve a desired outcome. If the aim was to add something, then creating the corresponding record in a database could be considered a correct action. Without more context, however, the problem remains unsolved due to missing specifics about what needs to be deleted or added.\n",
            "\n",
            "The final answer is: Response: We cannot proceed with a deletion operation as we do not know what is being referred to as \"this\". The action to create records in a database could be considered if you meant to refer to adding something.\n",
            "\n",
            "Category: Clarification Needed\n",
            "Action Taken: No action was taken\n",
            "Result: [No result as the problem remains unsolved]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Processed Response:\n",
            "Result: [Desired outcome for the user]\n",
            "\n",
            "## Step 1: Determine the type of action required.\n",
            "The user is requesting to delete something, which implies they are looking for an action to perform on a specific entity.\n",
            "\n",
            "## Step 2: Analyze the context provided by the user's query.\n",
            "In this case, there's no explicit mention of what \"this\" refers to. However, given the nature of queries like \"Please delete this\", it seems likely that the query pertains to a database or data storage system where something has been added (or \"created\") rather than removed from storage.\n",
            "\n",
            "## Step 3: Decide whether there is any relevant information that can guide our decision.\n",
            "Since we don't have any additional details about the object or entity referred to as \"this\", we must assume that the context might not be clear. In typical scenarios, users provide context when deleting items, such as identifying them through their IDs or names.\n",
            "\n",
            "## Step 4: Based on the analysis and available information, decide on the best course of action.\n",
            "Given the lack of clarity around \"this\" and assuming the intent might be related to adding an item, the most reasonable assumption without further clarification would be to look at the data that has been added (\"this\").\n",
            "\n",
            "## Step 5: Provide a suggested course of action based on our analysis and consideration of possible outcomes.\n",
            "We cannot directly apply a deletion operation to achieve a desired outcome. If the aim was to add something, then creating the corresponding record in a database could be considered a correct action. Without more context, however, the problem remains unsolved due to missing specifics about what needs to be deleted or added.\n",
            "\n",
            "The final answer is: Response: We cannot proceed with a deletion operation as we do not know what is being referred to as \"this\". The action to create records in a database could be considered if you meant to refer to adding something.\n",
            "\n",
            "Category: Clarification Needed\n",
            "Action Taken: No action was taken\n",
            "Result: [No result as the problem remains unsolved]\n",
            "\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.agents import Tool, AgentExecutor\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import BaseSingleActionAgent\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from typing import Optional, List, Any, Union\n",
        "import re\n",
        "import logging\n",
        "\n",
        "# Set up basic logging configuration to track execution and debug issues\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class CustomAgent(BaseSingleActionAgent):\n",
        "    \"\"\"\n",
        "    Custom agent implementation that decides whether to perform actions or provide information.\n",
        "    Inherits from LangChain's BaseSingleActionAgent for integration with the LangChain framework.\n",
        "    \"\"\"\n",
        "    llm: Any  # The language model instance\n",
        "    tools: List[Tool]  # List of available tools for the agent to use\n",
        "\n",
        "    @property\n",
        "    def input_keys(self):\n",
        "        \"\"\"Define the expected input keys for the agent.\"\"\"\n",
        "        return [\"input\"]\n",
        "\n",
        "    def plan(self, intermediate_steps: List[tuple], **kwargs) -> Union[AgentAction, AgentFinish]:\n",
        "        \"\"\"\n",
        "        Determine the next action based on the input query.\n",
        "\n",
        "        Args:\n",
        "            intermediate_steps: List of previous actions and results (unused in this implementation)\n",
        "            kwargs: Must contain 'input' key with the user's query\n",
        "\n",
        "        Returns:\n",
        "            Either an AgentAction to perform a task or AgentFinish with the final answer\n",
        "        \"\"\"\n",
        "        query = kwargs[\"input\"]\n",
        "\n",
        "        # Get response from LLM based on the input query\n",
        "        response = self.llm._call(query)\n",
        "\n",
        "        # Parse response to decide if an action is needed\n",
        "        if \"Category: Actionable Task\" in response:\n",
        "            # If response indicates an actionable task, return AgentAction\n",
        "            return AgentAction(\"PerformAction\", query, response)\n",
        "        else:\n",
        "            # If response indicates an information request, return AgentFinish\n",
        "            return AgentFinish(\n",
        "                return_values={\"output\": response},\n",
        "                log=response,\n",
        "            )\n",
        "\n",
        "    async def aplan(self, intermediate_steps: List[tuple], **kwargs) -> Union[AgentAction, AgentFinish]:\n",
        "        \"\"\"Async planning method - not implemented in this version.\"\"\"\n",
        "        raise NotImplementedError(\"Async planning is not implemented for this agent\")\n",
        "\n",
        "class CustomHuggingFaceLLM(LLM):\n",
        "    \"\"\"\n",
        "    Custom LLM implementation that wraps a Hugging Face model pipeline.\n",
        "    Formats queries and responses in a structured way for consistent interaction.\n",
        "    \"\"\"\n",
        "    pipeline: Any  # The Hugging Face pipeline instance\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Identifier for the custom LLM type.\"\"\"\n",
        "        return \"custom_huggingface\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Process the input prompt through the language model.\n",
        "\n",
        "        Args:\n",
        "            prompt: User's input query\n",
        "            stop: Optional stop sequences (unused in this implementation)\n",
        "\n",
        "        Returns:\n",
        "            Structured response string containing the answer, category, and action\n",
        "        \"\"\"\n",
        "        # Format the prompt with explicit instructions for the model (You can customize it according to your desired format)\n",
        "        formatted_prompt = f\"\"\"Please analyze the following query and respond in the exact format shown:\n",
        "\n",
        "Query: {prompt}\n",
        "\n",
        "Provide your response in this exact format:\n",
        "Response: [Answer the user query]\n",
        "Category: [Must be one of Information Request/Actionable Task/Clarification Needed based on user query]\n",
        "Action Taken: [Action description in case of Actionable Task or None]\n",
        "\"\"\"\n",
        "\n",
        "        # Generate response using the Hugging Face pipeline\n",
        "        outputs = self.pipeline(\n",
        "            formatted_prompt,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,  # Controls randomness in generation\n",
        "            top_p=0.95,      # Nucleus sampling parameter\n",
        "            repetition_penalty=1.1,  # Reduces repetition in generated text\n",
        "            return_full_text=False\n",
        "        )\n",
        "\n",
        "        generated_text = outputs[0]['generated_text']\n",
        "\n",
        "        # Ensure the response follows the required format\n",
        "        if \"Response:\" not in generated_text:\n",
        "            generated_text = f\"Response: {generated_text}\"\n",
        "        if \"Category:\" not in generated_text:\n",
        "            generated_text += \"\\nCategory: Information Request\"\n",
        "        if \"Action Taken:\" not in generated_text:\n",
        "            generated_text += \"\\nAction Taken: None\"\n",
        "\n",
        "        return generated_text.strip()\n",
        "\n",
        "def setup_model():\n",
        "    \"\"\"\n",
        "    Initialize and configure the Hugging Face model and tokenizer.\n",
        "    Returns:\n",
        "        CustomHuggingFaceLLM instance ready for use\n",
        "    \"\"\"\n",
        "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Change to your preferred model\n",
        "\n",
        "    # Initialize tokenizer with authentication\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"]\n",
        "    )\n",
        "\n",
        "    # Initialize model with appropriate settings for available hardware\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\",  # Automatically choose the best device configuration\n",
        "        token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"]\n",
        "    )\n",
        "\n",
        "    # Configure tokenizer padding settings\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    # Ensure model config matches tokenizer settings\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    # Create the text generation pipeline\n",
        "    text_generation_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    return CustomHuggingFaceLLM(pipeline=text_generation_pipeline)\n",
        "\n",
        "def get_answer(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Process general questions using the LLM.\n",
        "\n",
        "    Args:\n",
        "        query: User's question or request\n",
        "\n",
        "    Returns:\n",
        "        Formatted response from the LLM\n",
        "    \"\"\"\n",
        "    response = llm._call(query)\n",
        "    return response\n",
        "\n",
        "def perform_action(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Handle actionable tasks based on the query.\n",
        "\n",
        "    Args:\n",
        "        query: Description of the action to perform\n",
        "\n",
        "    Returns:\n",
        "        Status message about the action performed\n",
        "    \"\"\"\n",
        "    # Placeholder for actual action implementation\n",
        "    return f\"Action performed: {query}\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main application loop that sets up the agent and processes user queries.\n",
        "    Handles model initialization, user interaction, and cleanup.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Initializing model...\")\n",
        "        global llm\n",
        "        llm = setup_model()\n",
        "\n",
        "        # Define available tools for the agent\n",
        "        tools = [\n",
        "            Tool(\n",
        "                name=\"GetAnswer\",\n",
        "                func=get_answer,\n",
        "                description=\"Use this for general questions and information requests.\"\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"PerformAction\",\n",
        "                func=perform_action,\n",
        "                description=\"Use this for actionable tasks that require specific actions.\"\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Initialize the custom agent with tools\n",
        "        agent = CustomAgent(\n",
        "            llm=llm,\n",
        "            tools=tools\n",
        "        )\n",
        "\n",
        "        # Create the agent executor with the specified configuration\n",
        "        agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "            agent=agent,\n",
        "            tools=tools,\n",
        "            verbose=True,\n",
        "            max_iterations=3  # Limit the number of tool uses per query\n",
        "        )\n",
        "\n",
        "        print(\"\\nAgent ready! Enter your queries (type 'quit' to exit)\")\n",
        "\n",
        "        # Main interaction loop\n",
        "        while True:\n",
        "            query = input(\"\\nEnter query: \").strip()\n",
        "            if query.lower() in ['quit', 'exit']:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                print(\"Processing query...\")\n",
        "                response = agent_executor.invoke({\"input\": query})\n",
        "\n",
        "                print(\"\\nProcessed Response:\")\n",
        "                print(response[\"output\"])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError: {str(e)}\")\n",
        "                print(\"Please try again with a different query.\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nExiting...\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFatal error: {str(e)}\")\n",
        "    finally:\n",
        "        # Clean up GPU memory if available\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
